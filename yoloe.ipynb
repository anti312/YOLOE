{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-xiQ5LImtXw"
      },
      "outputs": [],
      "source": [
        "# Download Dependencies\n",
        "\n",
        "!pip install git+https://github.com/THU-MIG/yoloe.git#subdirectory=third_party/CLIP\n",
        "!pip install git+https://github.com/THU-MIG/yoloe.git#subdirectory=third_party/ml-mobileclip\n",
        "!pip install git+https://github.com/THU-MIG/yoloe.git#subdirectory=third_party/lvis-api\n",
        "!pip install git+https://github.com/THU-MIG/yoloe.git\n",
        "\n",
        "!wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_blt.pt\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "hf_hub_download(repo_id=\"jameslahm/yoloe\", filename=\"yoloe-11s-seg.pt\", local_dir='.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the YOLOE model from the ultralytics library\n",
        "from ultralytics import YOLOE\n",
        "\n",
        "# Load the YOLOE model\n",
        "model = YOLOE(\"yoloe-11s-seg.pt\").cuda()\n",
        "\n",
        "# Define the class names that the model will recognize\n",
        "names = [\"suitcase\"]\n",
        "model.set_classes(names, model.get_text_pe(names))\n"
      ],
      "metadata": {
        "id": "2JXPDgdhnsZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " processes and predicts images from a folder and displays the results without calculating Intersection over Union (IoU)."
      ],
      "metadata": {
        "id": "qZJiRt03KGDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the model without IoU calculation\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "dataset_folder = \"/content/dataset\"\n",
        "for filename in os.listdir(dataset_folder):\n",
        "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "            image_path = os.path.join(dataset_folder, filename)\n",
        "            results = model.predict(image_path)\n",
        "            results[0].show()"
      ],
      "metadata": {
        "id": "RGigkqb9MzIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " performs object detection evaluation by comparing model predictions to ground truth annotations using the IoU metric, and reports on the performance."
      ],
      "metadata": {
        "id": "4THFYBY5KgFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# Convert bounding box from xywhn format (normalized center coordinates) to xyxy format (absolute coordinates)\n",
        "def xywhn_to_xyxy(bbox, img_width, img_height):\n",
        "    x_c, y_c, w, h = bbox\n",
        "    x1 = (x_c - w / 2) * img_width\n",
        "    y1 = (y_c - h / 2) * img_height\n",
        "    x2 = (x_c + w / 2) * img_width\n",
        "    y2 = (y_c + h / 2) * img_height\n",
        "    return [x1, y1, x2, y2]\n",
        "\n",
        "# Compute IoU between two bounding boxes\n",
        "def compute_iou(boxA, boxB):\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "\n",
        "    boxAArea = max(0, boxA[2] - boxA[0]) * max(0, boxA[3] - boxA[1])\n",
        "    boxBArea = max(0, boxB[2] - boxB[0]) * max(0, boxB[3] - boxB[1])\n",
        "\n",
        "    iou = interArea / (boxAArea + boxBArea - interArea + 1e-6)  # Small epsilon to avoid division by zero\n",
        "    return iou\n",
        "\n",
        "\n",
        "dataset_folder = \"/content/dataset\"\n",
        "annotations_folder = \"/content/dataset/annotations\"\n",
        "\n",
        "IOU_THRESHOLD = 0.5\n",
        "total_pred = 0  # Count of all predicted boxes\n",
        "all_ious = []   # List to store IoUs for valid matches\n",
        "\n",
        "# Iterate through the images in the dataset folder\n",
        "for filename in os.listdir(dataset_folder):\n",
        "    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "        image_path = os.path.join(dataset_folder, filename)  # Full path to the image file\n",
        "        image = Image.open(image_path)\n",
        "        width, height = image.size\n",
        "\n",
        "        # Make predictions on the image using the model (set confidence threshold to 0.26)\n",
        "        results = model.predict(image_path, conf=0.26)\n",
        "        pred_boxes = results[0].boxes.xywhn.cpu().numpy()  # Predicted bounding boxes (xywhn format)\n",
        "        results[0].show()\n",
        "\n",
        "        # Load the ground truth annotations for the current image\n",
        "        annotation_path = os.path.join(annotations_folder, os.path.splitext(filename)[0] + \".txt\")\n",
        "        if not os.path.exists(annotation_path):\n",
        "            print(f\"No annotation for {filename}\")\n",
        "            continue\n",
        "\n",
        "        with open(annotation_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "        gt_boxes = [list(map(float, line.strip().split()[1:])) for line in lines]  # Extract ground truth bounding boxes\n",
        "\n",
        "        # Convert both predicted and ground truth boxes to xyxy format\n",
        "        pred_xyxy = [xywhn_to_xyxy(box, width, height) for box in pred_boxes]\n",
        "        total_pred += len(pred_xyxy)\n",
        "        gt_xyxy = [xywhn_to_xyxy(box, width, height) for box in gt_boxes]\n",
        "\n",
        "        matched_gt = set()  # Set to keep track of matched ground truth boxes\n",
        "\n",
        "        # Compare predicted boxes with ground truth boxes\n",
        "        for i, pbox in enumerate(pred_xyxy):\n",
        "            best_iou = 0\n",
        "            best_j = -1\n",
        "            for j, gbox in enumerate(gt_xyxy):\n",
        "                if j in matched_gt:\n",
        "                    continue\n",
        "                iou = compute_iou(pbox, gbox)\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_j = j\n",
        "\n",
        "            # Check if the best IoU is above the threshold\n",
        "            if best_iou >= IOU_THRESHOLD:\n",
        "                all_ious.append(best_iou)\n",
        "                matched_gt.add(best_j)\n",
        "                print(f\"{filename} - Match: Pred {i} ‚Üî GT {best_j} | IoU: {best_iou:.4f}\")\n",
        "            else:\n",
        "                print(f\"{filename} - Pred {i} ‚Üí No match (max IoU = {best_iou:.4f})\")\n",
        "\n",
        "# Calculate and print the mean IoU across all matches\n",
        "if all_ious:\n",
        "    mean_iou = sum(all_ious) / total_pred\n",
        "    print(f\"\\nüìä Mean IoU over all matches: {mean_iou:.4f} (with {total_pred} predictions and {len(all_ious)} matches)\")\n",
        "else:\n",
        "    print(\"\\n‚ùå No valid bounding box matches found.\")\n"
      ],
      "metadata": {
        "id": "wrQA1q4IOvfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processes a video, detects objects using the YOLOe model, tracks changes in detection over time, and creates a new video with a timeline indicating when events (detection changes) occur. The output video has annotations on the objects and a timeline at the top that highlights these events in real-time. Additionally, key frames are saved when significant changes in object detection are observed."
      ],
      "metadata": {
        "id": "77jcvUNoKj-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://media.roboflow.com/supervision/video-examples/suitcases-1280x720.mp4"
      ],
      "metadata": {
        "id": "b22yHwgcKEq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv\n",
        "from ultralytics import YOLOE\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# === CONFIG ===\n",
        "SOURCE_VIDEO_PATH = \"/content/suitcases-1280x720.mp4\"\n",
        "TARGET_VIDEO_PATH = \"/content/suitcases-result.mp4\"\n",
        "TIMELINE_HEIGHT = 50\n",
        "FRAME_SAVE_DIR = \"key_frames\"\n",
        "CONFIDENCE_THRESHOLD = 0.1\n",
        "\n",
        "# === VIDEO SETUP ===\n",
        "original_video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "total_frames = original_video_info.total_frames\n",
        "frame_generator = list(sv.get_video_frames_generator(SOURCE_VIDEO_PATH))  # convert to list for two passes\n",
        "\n",
        "# Modifica le dimensioni del video per includere la timeline\n",
        "video_info = sv.VideoInfo(\n",
        "    width=original_video_info.width,\n",
        "    height=original_video_info.height + TIMELINE_HEIGHT,\n",
        "    fps=original_video_info.fps\n",
        ")\n",
        "\n",
        "# === STORAGE ===\n",
        "os.makedirs(FRAME_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# === FASE 1: analisi eventi per timeline ===\n",
        "print(\"Analisi preliminare del video...\")\n",
        "event_frame_indices = []\n",
        "prev_detection_count = None\n",
        "\n",
        "for index, frame in enumerate(tqdm(frame_generator, desc=\"Scansione eventi\")):\n",
        "    results = model.predict(frame, conf=CONFIDENCE_THRESHOLD, verbose=False)\n",
        "    detections = sv.Detections.from_ultralytics(results[0])\n",
        "    current_count = len(detections)\n",
        "\n",
        "    if prev_detection_count is not None and current_count != prev_detection_count:\n",
        "        event_frame_indices.append(index)\n",
        "\n",
        "    prev_detection_count = current_count\n",
        "\n",
        "# === Calcolo posizioni eventi nella timeline ===\n",
        "event_positions = set()\n",
        "for idx in event_frame_indices:\n",
        "    x_pos = int((idx / total_frames) * video_info.width)\n",
        "    event_positions.add(x_pos)\n",
        "\n",
        "# === FASE 2: generazione video con timeline sopra ===\n",
        "print(\"Rendering video finale...\")\n",
        "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "    for index, frame in enumerate(tqdm(frame_generator, desc=\"Rendering frames\")):\n",
        "        results = model.predict(frame, conf=CONFIDENCE_THRESHOLD, verbose=False)\n",
        "        detections = sv.Detections.from_ultralytics(results[0])\n",
        "\n",
        "        # Annotazioni\n",
        "        annotated_image = frame.copy()\n",
        "        annotated_image = sv.ColorAnnotator().annotate(scene=annotated_image, detections=detections)\n",
        "        annotated_image = sv.BoxAnnotator().annotate(scene=annotated_image, detections=detections)\n",
        "        annotated_image = sv.LabelAnnotator().annotate(scene=annotated_image, detections=detections)\n",
        "\n",
        "        # Salva keyframe e aggiungi segnale visivo se c'√® evento\n",
        "        if index in event_frame_indices:\n",
        "            Image.fromarray(annotated_image).save(f\"{FRAME_SAVE_DIR}/frame_{index}.jpg\")\n",
        "\n",
        "        # Crea timeline bianca\n",
        "        timeline = np.ones((TIMELINE_HEIGHT, video_info.width, 3), dtype=np.uint8) * 255\n",
        "\n",
        "        # Disegna tutti gli eventi rilevati (in rosso)\n",
        "        for x in event_positions:\n",
        "            cv2.line(timeline, (x, 0), (x, TIMELINE_HEIGHT), (0, 0, 255), 2)\n",
        "\n",
        "        # Posizione attuale (in grigio)\n",
        "        progress_pos = int((index / total_frames) * video_info.width)\n",
        "        cv2.line(timeline, (progress_pos, 0), (progress_pos, TIMELINE_HEIGHT), (150, 150, 150), 2)\n",
        "\n",
        "        # Unisce timeline e frame (timeline sopra)\n",
        "        full_frame = np.vstack([timeline, annotated_image])\n",
        "\n",
        "        sink.write_frame(full_frame)\n"
      ],
      "metadata": {
        "id": "6gIDa3tWy95k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renove dataset folder\n",
        "!rm -rf /content/key_frames"
      ],
      "metadata": {
        "id": "oUQcfpcijHSI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}